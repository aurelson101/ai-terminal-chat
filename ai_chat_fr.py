#!/usr/bin/env python3
"""
Terminal AI Chat - Un clone de Warp pour Linux
Permet d'utiliser diffÃ©rents LLM (locaux ou API) directement dans le terminal
"""

import sys
import json
import requests
import subprocess
import argparse
from pathlib import Path
from typing import Dict, Any
import shutil
from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel
from rich.prompt import Prompt
from rich.table import Table
import pyperclip

def safe_prompt(prompt_text, **kwargs):
    """Prompt sÃ©curisÃ© avec gestion des interruptions clavier et EOF"""
    try:
        return Prompt.ask(prompt_text, **kwargs)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Configuration annulÃ©e par l'utilisateur.")
        print("ðŸ’¡ Vous pouvez relancer la configuration plus tard avec: chat --config")
        sys.exit(0)
    except EOFError:
        print("\n\nâš ï¸  Flux d'entrÃ©e fermÃ© de maniÃ¨re inattendue.")
        print("ðŸ’¡ Veuillez lancer la configuration dans un terminal interactif: chat --config")
        sys.exit(0)

class AITerminalChat:
    def __init__(self):
        self.console = Console()
        self.config_dir = Path.home() / ".ai_terminal_chat"
        self.config_file = self.config_dir / "config.json"
        self.config = self.load_config()
        
    def load_config(self) -> Dict[str, Any]:
        """Charge la configuration ou crÃ©e une configuration par dÃ©faut"""
        if not self.config_dir.exists():
            self.config_dir.mkdir(parents=True)
            
        if not self.config_file.exists():
            return self.setup_initial_config()
        
        try:
            with open(self.config_file, 'r') as f:
                content = f.read().strip()
                if not content:  # Fichier vide
                    self.console.print("[dim]Fichier de configuration vide, crÃ©ation d'une nouvelle configuration...[/dim]")
                    return self.setup_initial_config()
                return json.loads(content)
        except (json.JSONDecodeError, FileNotFoundError) as e:
            self.console.print(f"[bold]Fichier de configuration corrompu ({e}), crÃ©ation d'une nouvelle configuration...[/bold]")
            # Sauvegarder l'ancien fichier corrompu
            if self.config_file.exists():
                backup_file = self.config_file.with_suffix('.json.backup')
                self.config_file.rename(backup_file)
                self.console.print(f"[dim]Ancien fichier sauvegardÃ© en tant que {backup_file}[/dim]")
            return self.setup_initial_config()
        except Exception as e:
            self.console.print(f"[bold]Erreur lors du chargement de la config: {e}[/bold]")
            return self.setup_initial_config()
    
    def save_config(self):
        """Sauvegarde la configuration"""
        try:
            with open(self.config_file, 'w') as f:
                json.dump(self.config, f, indent=2)
        except Exception as e:
            self.console.print(f"[bold]Erreur lors de la sauvegarde de la configuration: {e}[/bold]")
    
    def setup_initial_config(self) -> Dict[str, Any]:
        """Configuration initiale interactive"""
        self.console.print(Panel.fit("ðŸ¤– Configuration initiale de AI Terminal Chat", style="bold"))
        
        # Types de LLM disponibles
        llm_types = {
            "1": {"name": "Ollama (Local)", "type": "ollama"},
            "2": {"name": "LM Studio (Local)", "type": "lmstudio"},
            "3": {"name": "OpenAI API", "type": "openai"},
            "4": {"name": "OpenRouter API", "type": "openrouter"},
            "5": {"name": "Anthropic API", "type": "anthropic"},
            "6": {"name": "Groq API", "type": "groq"}
        }
        
        # Affichage des options
        table = Table(title="Types de LLM disponibles")
        table.add_column("Option", style="bold")
        table.add_column("Description", style="default")
        table.add_column("Type", style="dim")
        
        for key, value in llm_types.items():
            table.add_row(key, value["name"], value["type"])
        
        self.console.print(table)
        
        try:
            choice = safe_prompt("Choisissez votre type de LLM", choices=list(llm_types.keys()))
        except KeyboardInterrupt:
            print("\n\nâš ï¸  Configuration annulÃ©e par l'utilisateur.")
            print("ðŸ’¡ Vous pouvez relancer la configuration plus tard avec: chat --config")
            sys.exit(0)
        
        selected_llm = llm_types[choice]
        
        config = {
            "llm_type": selected_llm["type"],
            "llm_name": selected_llm["name"]
        }
        
        # Configuration spÃ©cifique selon le type
        if selected_llm["type"] == "ollama":
            config.update(self.setup_ollama())
        elif selected_llm["type"] == "lmstudio":
            config.update(self.setup_lmstudio())
        elif selected_llm["type"] == "openai":
            config.update(self.setup_openai())
        elif selected_llm["type"] == "openrouter":
            config.update(self.setup_openrouter())
        elif selected_llm["type"] == "anthropic":
            config.update(self.setup_anthropic())
        elif selected_llm["type"] == "groq":
            config.update(self.setup_groq())
        
        # Assigner la config avant de sauvegarder
        self.config = config
        
        # Sauvegarder la config
        self.save_config()
        return config
    
    def setup_ollama(self) -> Dict[str, Any]:
        """Configuration Ollama"""
        self.console.print("[bold]Configuration Ollama[/bold]")
        
        # VÃ©rifier si Ollama est installÃ©
        if not shutil.which("ollama"):
            self.console.print("[bold]Ollama n'est pas installÃ©. Installez-le depuis https://ollama.ai[/bold]")
            sys.exit(1)
        
        # Lister les modÃ¨les disponibles
        try:
            result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
            if result.returncode == 0:
                self.console.print("[bold]ModÃ¨les Ollama disponibles:[/bold]")
                self.console.print(result.stdout)
            else:
                self.console.print("[dim]Aucun modÃ¨le Ollama trouvÃ©.[/dim]")
        except Exception:
            pass
        
        model = safe_prompt("ModÃ¨le Ollama Ã  utiliser", default="llama2")
        base_url = safe_prompt("URL de base Ollama", default="http://localhost:11434")
        
        return {
            "model": model,
            "base_url": base_url
        }
    
    def setup_lmstudio(self) -> Dict[str, Any]:
        """Configuration LM Studio"""
        self.console.print("[bold]Configuration LM Studio[/bold]")
        
        model = safe_prompt("Nom du modÃ¨le LM Studio", default="local-model")
        base_url = safe_prompt("URL de base LM Studio", default="http://localhost:1234")
        
        return {
            "model": model,
            "base_url": base_url
        }
    
    def setup_openai(self) -> Dict[str, Any]:
        """Configuration OpenAI"""
        self.console.print("[bold]Configuration OpenAI[/bold]")
        
        # ModÃ¨les recommandÃ©s
        models = {
            "1": "gpt-4o-mini (RecommandÃ© pour le code)",
            "2": "gpt-4o (Plus puissant)",
            "3": "gpt-3.5-turbo (Ã‰conomique)",
            "4": "Autre (saisie manuelle)"
        }
        
        table = Table(title="ModÃ¨les OpenAI")
        table.add_column("Option", style="bold")
        table.add_column("ModÃ¨le", style="default")
        
        for key, value in models.items():
            table.add_row(key, value)
        
        self.console.print(table)
        
        choice = safe_prompt("Choisissez un modÃ¨le", choices=list(models.keys()))
        
        if choice == "1":
            model = "gpt-4o-mini"
        elif choice == "2":
            model = "gpt-4o"
        elif choice == "3":
            model = "gpt-3.5-turbo"
        else:
            model = safe_prompt("Nom du modÃ¨le")
        
        api_key = safe_prompt("ClÃ© API OpenAI", password=True)
        
        return {
            "model": model,
            "api_key": api_key,
            "base_url": "https://api.openai.com/v1"
        }
    
    def setup_openrouter(self) -> Dict[str, Any]:
        """Configuration OpenRouter"""
        self.console.print("[bold]Configuration OpenRouter[/bold]")
        
        # ModÃ¨les gratuits et payants
        free_models = {
            "1": "microsoft/phi-4-reasoning-plus:free (Gratuit - Nouveau ! Excellent pour le code)",
            "2": "google/gemma-2-9b-it:free (Gratuit - GÃ©nÃ©ral)",
            "3": "meta-llama/llama-3.1-8b-instruct:free (Gratuit - GÃ©nÃ©ral)"
        }
        
        paid_models = {
            "4": "anthropic/claude-3.5-sonnet (Payant - Excellent pour le code)",
            "5": "openai/gpt-4o (Payant - TrÃ¨s bon pour le code)",
            "6": "google/gemini-pro-1.5 (Payant - Bon pour le code)",
            "7": "meta-llama/llama-3.1-70b-instruct (Payant - Bon pour le code)",
            "8": "Autre (saisie manuelle)"
        }
        
        # Affichage des modÃ¨les gratuits
        table_free = Table(title="ModÃ¨les OpenRouter GRATUITS")
        table_free.add_column("Option", style="bold")
        table_free.add_column("ModÃ¨le", style="default")
        
        for key, value in free_models.items():
            table_free.add_row(key, value)
        
        self.console.print(table_free)
        
        # Affichage des modÃ¨les payants
        table_paid = Table(title="ModÃ¨les OpenRouter PAYANTS (Performance supÃ©rieure)")
        table_paid.add_column("Option", style="bold")
        table_paid.add_column("ModÃ¨le", style="dim")
        
        for key, value in paid_models.items():
            table_paid.add_row(key, value)
        
        self.console.print(table_paid)
        
        all_models = {**free_models, **paid_models}
        choice = safe_prompt("Choisissez un modÃ¨le", choices=list(all_models.keys()))
        
        model_mapping = {
            "1": "microsoft/phi-4-reasoning-plus:free",
            "2": "microsoft/phi-3-mini-128k-instruct:free",
            "3": "google/gemma-2-9b-it:free", 
            "4": "meta-llama/llama-3.1-8b-instruct:free",
            "5": "anthropic/claude-3.5-sonnet",
            "6": "openai/gpt-4o",
            "7": "google/gemini-pro-1.5",
            "8": "meta-llama/llama-3.1-70b-instruct"
        }
        
        if choice in model_mapping:
            model = model_mapping[choice]
        else:
            model = safe_prompt("Nom du modÃ¨le")
        
        api_key = safe_prompt("ClÃ© API OpenRouter", password=True)
        
        return {
            "model": model,
            "api_key": api_key,
            "base_url": "https://openrouter.ai/api/v1"
        }
    
    def setup_anthropic(self) -> Dict[str, Any]:
        """Configuration Anthropic"""
        self.console.print("[bold]Configuration Anthropic[/bold]")
        
        models = {
            "1": "claude-3-5-sonnet-20241022 (RecommandÃ© pour le code)",
            "2": "claude-3-haiku-20240307 (Plus rapide)",
            "3": "Autre (saisie manuelle)"
        }
        
        table = Table(title="ModÃ¨les Anthropic")
        table.add_column("Option", style="bold")
        table.add_column("ModÃ¨le", style="default")
        
        for key, value in models.items():
            table.add_row(key, value)
        
        self.console.print(table)
        
        choice = safe_prompt("Choisissez un modÃ¨le", choices=list(models.keys()))
        
        if choice == "1":
            model = "claude-3-5-sonnet-20241022"
        elif choice == "2":
            model = "claude-3-haiku-20240307"
        else:
            model = safe_prompt("Nom du modÃ¨le")
        
        api_key = safe_prompt("ClÃ© API Anthropic", password=True)
        
        return {
            "model": model,
            "api_key": api_key,
            "base_url": "https://api.anthropic.com"
        }
    
    def setup_groq(self) -> Dict[str, Any]:
        """Configuration Groq"""
        self.console.print("[bold]Configuration Groq[/bold]")
        
        models = {
            "1": "llama-3.1-70b-versatile (Gratuit - Excellent pour le code)",
            "2": "llama-3.1-8b-instant (Gratuit - Rapide)",
            "3": "mixtral-8x7b-32768 (Gratuit - Bon pour le code)",
            "4": "Autre (saisie manuelle)"
        }
        
        table = Table(title="ModÃ¨les Groq (API gratuite avec limites)")
        table.add_column("Option", style="bold")
        table.add_column("ModÃ¨le", style="default")
        
        for key, value in models.items():
            table.add_row(key, value)
        
        self.console.print(table)
        
        choice = safe_prompt("Choisissez un modÃ¨le", choices=list(models.keys()))
        
        if choice == "1":
            model = "llama-3.1-70b-versatile"
        elif choice == "2":
            model = "llama-3.1-8b-instant"
        elif choice == "3":
            model = "mixtral-8x7b-32768"
        else:
            model = safe_prompt("Nom du modÃ¨le")
        
        api_key = safe_prompt("ClÃ© API Groq", password=True)
        
        return {
            "model": model,
            "api_key": api_key,
            "base_url": "https://api.groq.com/openai/v1"
        }
    
    def send_message(self, message: str) -> str:
        """Envoie un message au LLM configurÃ©"""
        try:
            if self.config["llm_type"] in ["ollama", "lmstudio"]:
                return self.send_to_local_llm(message)
            else:
                return self.send_to_api_llm(message)
        except requests.exceptions.HTTPError as e:
            # Erreur HTTP spÃ©cifique
            if e.response.status_code == 404:
                return f"âŒ Erreur 404: Endpoint API non trouvÃ©. VÃ©rifiez la configuration de votre LLM.\nURL utilisÃ©e: {e.response.url}\nType LLM: {self.config['llm_type']}"
            elif e.response.status_code == 401:
                return f"âŒ Erreur 401: ClÃ© API invalide ou expirÃ©e pour {self.config['llm_type']}"
            elif e.response.status_code == 429:
                return f"âŒ Erreur 429: Limite de taux dÃ©passÃ©e pour {self.config['llm_type']}. Attendez un moment."
            else:
                return f"âŒ Erreur HTTP {e.response.status_code}: {e.response.text}"
        except requests.exceptions.ConnectionError:
            return f"âŒ Erreur de connexion. VÃ©rifiez votre connexion internet et l'URL de base: {self.config.get('base_url', 'non configurÃ©e')}"
        except requests.exceptions.Timeout:
            return f"âŒ Timeout: Le LLM {self.config['llm_type']} met trop de temps Ã  rÃ©pondre"
        except KeyError as e:
            return f"âŒ Erreur de configuration: clÃ© manquante {e}. Utilisez 'config' pour reconfigurer."
        except Exception as e:
            return f"âŒ Erreur lors de l'envoi du message: {str(e)}"
    
    def send_to_local_llm(self, message: str) -> str:
        """Envoi vers un LLM local (Ollama/LMStudio)"""
        url = f"{self.config['base_url']}/api/generate"
        
        data = {
            "model": self.config["model"],
            "prompt": message,
            "stream": False
        }
        
        response = requests.post(url, json=data, timeout=120)
        response.raise_for_status()
        
        if self.config["llm_type"] == "ollama":
            return response.json()["response"]
        else:  # LMStudio
            return response.json()["choices"][0]["text"]
    
    def send_to_api_llm(self, message: str) -> str:
        """Envoi vers une API externe"""
        headers = {
            "Content-Type": "application/json"
        }
        
        if self.config["llm_type"] == "anthropic":
            headers["x-api-key"] = self.config["api_key"]
            headers["anthropic-version"] = "2023-06-01"
            
            data = {
                "model": self.config["model"],
                "max_tokens": 4096,
                "messages": [{"role": "user", "content": message}]
            }
            
            response = requests.post(
                f"{self.config['base_url']}/v1/messages",
                headers=headers,
                json=data,
                timeout=120
            )
            response.raise_for_status()
            return response.json()["content"][0]["text"]
        
        else:  # OpenAI, OpenRouter, Groq (compatible OpenAI)
            headers["Authorization"] = f"Bearer {self.config['api_key']}"
            
            # Ajouter des en-tÃªtes spÃ©cifiques pour OpenRouter (SELON LA DOC OFFICIELLE)
            if self.config["llm_type"] == "openrouter":
                headers["HTTP-Referer"] = "https://github.com/ai-terminal-chat"
                headers["X-Title"] = "AI Terminal Chat"
            
            data = {
                "model": self.config["model"],
                "messages": [{"role": "user", "content": message}],
                "max_tokens": 4096,
                "temperature": 0.7
            }
            
            # CORRECTION CLÃ‰E : OpenRouter veut data=json.dumps(), pas json=data
            if self.config["llm_type"] == "openrouter":
                import json
                response = requests.post(
                    f"{self.config['base_url']}/chat/completions",
                    headers=headers,
                    data=json.dumps(data),  # <-- VOICI LA CORRECTION !
                    timeout=120
                )
            else:
                # Groq et OpenAI utilisent json=data normalement
                response = requests.post(
                    f"{self.config['base_url']}/chat/completions",
                    headers=headers,
                    json=data,
                    timeout=120
                )
            
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
    
    def format_response(self, response: str):
        """Formate la rÃ©ponse avec Rich (markdown, code, etc.)"""
        try:
            # DÃ©tection de code dans la rÃ©ponse
            if "```" in response:
                self.console.print(Markdown(response))
            else:
                # Affichage simple si pas de markdown
                self.console.print(Panel(response, title="ðŸ¤– RÃ©ponse IA", style="default"))
        except Exception:
            # Fallback en cas d'erreur de formatage
            self.console.print(response)
    
    def start_chat(self):
        """DÃ©marre le chat interactif"""
        self.console.print(Panel.fit(
            f"ðŸ¤– AI Terminal Chat\n"
            f"LLM: {self.config['llm_name']} ({self.config['model']})\n"
            f"Tapez 'quit', 'exit' ou Ctrl+C pour quitter\n"
            f"Tapez 'copy' pour copier la derniÃ¨re rÃ©ponse\n"
            f"Tapez 'config' pour reconfigurer",
            style="bold"
        ))
        
        last_response = ""
        
        try:
            while True:
                # Prompt utilisateur
                user_input = safe_prompt("\n[bold]Vous[/bold]")
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    break
                elif user_input.lower() == 'copy':
                    if last_response:
                        try:
                            pyperclip.copy(last_response)
                            self.console.print("[bold]âœ“ RÃ©ponse copiÃ©e dans le presse-papiers[/bold]")
                        except Exception:
                            self.console.print("[bold]Erreur: impossible de copier dans le presse-papiers[/bold]")
                    else:
                        self.console.print("[dim]Aucune rÃ©ponse Ã  copier[/dim]")
                    continue
                elif user_input.lower() == 'config':
                    self.config = self.setup_initial_config()
                    continue
                
                # Indicateur de chargement
                with self.console.status("[bold]ðŸ¤” RÃ©flexion en cours..."):
                    response = self.send_message(user_input)
                
                last_response = response
                self.format_response(response)
                
        except KeyboardInterrupt:
            self.console.print("\n[dim]Au revoir! ðŸ‘‹[/dim]")
        except Exception as e:
            self.console.print(f"[bold]Erreur: {e}[/bold]")

def main():
    parser = argparse.ArgumentParser(description="AI Terminal Chat - Clone de Warp pour Linux")
    parser.add_argument("--config", action="store_true", help="Reconfigurer le LLM")
    args = parser.parse_args()
    
    chat = AITerminalChat()
    
    if args.config:
        chat.config = chat.setup_initial_config()
    
    chat.start_chat()

if __name__ == "__main__":
    main()
